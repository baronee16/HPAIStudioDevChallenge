{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3f7a87-1faf-4f44-8b89-b7be46ffcc2b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dca25-6a47-4674-aa23-8dd9e7afbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc18236",
   "metadata": {},
   "source": [
    "## Define Constants and Paths and Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c96f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "REGISTER_NAME = \"Shakespeare_Model\"\n",
    "EXPERIMENT_NAME = \"Shakespeare Text Generation\"\n",
    "RUN_NAME = \"Shakespeare_main\"\n",
    "\n",
    "\n",
    "# Set up the paths\n",
    "DATA_PATH = \"shakespeare.txt\"\n",
    "MODEL_PATH = \"models/dict_torch_rnn_model.pt\"\n",
    "MODEL_DECODER_PATH = \"models/decoder.pt\"\n",
    "MODEL_ENCODER_PATH = \"models/encoder.pt\"\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cccd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging module with desired format and level\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create a logger for this notebook\n",
    "logger = logging.getLogger('deployment-notebook')\n",
    "logger.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89849458-6614-41f7-916c-ca38aec363eb",
   "metadata": {},
   "source": [
    "## Get Text Data\n",
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts.\n",
    "\n",
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e89ba8-1e88-450e-bea4-4a0a99793bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH,'r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e8c88-c69f-416a-9ac0-04f049988fc2",
   "metadata": {},
   "source": [
    "## Loading Model\n",
    "\n",
    "The models are available at the [models](models/) folder, where:\n",
    " - [Tensorflow Jupyter Notebook](RNN_for_text_generation_TF.ipynb): `tf_rnn_model.h5`\n",
    " - [PyTorch Jupyter Notebook](RNN_for_text_generation_Torch.): `dict_torch_rnn_model.pt`. Also includes the `decoder.pt` and `encoder.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ac375-2b3d-431c-a186-d1372e106cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        \"\"\"Initializes CharModel\n",
    "\n",
    "        Args:\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            all_chars: Set of unique characters found in the text.\n",
    "            num_hidden: Number of hidden layers. Defaults to 256.\n",
    "            num_layers: Number of layers. Defaults to 4.\n",
    "            drop_prob: Regularization technique to prevent overfitting. Defaults to 0.5.\n",
    "            use_gpu: If the model uses GPU. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = torch.load(decoder)\n",
    "        self.encoder = torch.load(encoder)\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (_type_): _description_\n",
    "            hidden (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        lstm_output, hidden = self.lstm(x, hidden)       \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            batch_size (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df33904-7221-4be1-846d-6f7a24c58cb9",
   "metadata": {},
   "source": [
    "# MLFlow - Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d2faa-e9d5-45b3-a862-8c6136bbe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            context (_type_): _description_\n",
    "        \"\"\"\n",
    "        self.model = CharModel(\n",
    "                        all_chars=all_characters,\n",
    "                        num_hidden=512,\n",
    "                        num_layers=3,\n",
    "                        drop_prob=0.5,\n",
    "                        use_gpu=False,\n",
    "                        decoder=context.artifacts['decoder'],\n",
    "                        encoder=context.artifacts['encoder']\n",
    "                                           \n",
    "                    )\n",
    "\n",
    "\n",
    "        self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "        self.model.eval()\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            char (_type_): _description_\n",
    "            hidden (_type_, optional): _description_. Defaults to None.\n",
    "            k (int, optional): _description_. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        encoded_text = self.model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        inputs = inputs.cpu()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = self.model(inputs, hidden)    \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        probs = probs.cpu()\n",
    "\n",
    "        \n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "        return self.model.decoder[char], hidden\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "\n",
    "        self.model.cpu()\n",
    "            \n",
    "        self.model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = self.model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "    \n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "            \n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        initial_word = model_input['initial_word'][0]\n",
    "        size = model_input['size'][0]\n",
    "        output = self.generate_text(seed=initial_word, size=size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder, demo_folder=\"demo\"): \n",
    "        input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"initial_word\"),\n",
    "                ColSpec(\"long\", \"size\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"generated_text\")\n",
    "            ]\n",
    "        )\n",
    "      \n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "             \n",
    "        requirements = [\n",
    "            \"torch\",\n",
    "            \"numpy\"\n",
    "        ]\n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_state_dict,\n",
    "            python_model=cls(),\n",
    "            artifacts={\n",
    "                \"model_state_dict\": model_state_dict, \n",
    "                'decoder': decoder, \n",
    "                'encoder': encoder, \n",
    "                \"demo\": demo_folder},\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88808f17-797b-4273-86a8-ec4be7b67bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name= EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28b95-5dd5-4f36-adb3-258a85edc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf659ed-12e3-4941-b46f-d8cacfdfa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_name = REGISTER_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344da6ed-dd4a-4063-86ac-f6f0168ffb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name = RUN_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, MODEL_DECODER_PATH, MODEL_ENCODER_PATH)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4e7fa-fda9-422d-bff6-b077b6c8d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cf27e-2a84-469c-8fe7-ab2508a4a4d4",
   "metadata": {},
   "source": [
    "## Testing registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caeb0bd-e530-461f-a30a-1a918f85a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": 'Love ', \"size\": 100}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
