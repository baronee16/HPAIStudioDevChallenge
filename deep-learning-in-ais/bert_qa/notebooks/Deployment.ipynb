{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0a5604",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this notebook, our objective is to deploy Hugging Face Model of Bert_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba89ec4",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Install requirements and Imports Dependencies\n",
    "- Define Constants and Paths and Configure Logging\n",
    "- Model Load\n",
    "- Model Registry\n",
    "- Testing latest model registred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c2be1-7e41-4318-b54b-74c0860ac597",
   "metadata": {},
   "source": [
    "# Install requirements and Imports Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853f387-e023-46f8-bb01-a7db1049e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "\n",
    "# Transformers\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda96c33",
   "metadata": {},
   "source": [
    "# Define Constants and Paths and Configure Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "MODEL_PERSONAL_NAME = \"morgana-rodrigues/bert_qa\"\n",
    "EXPERIMENT_NAME = \"BERT model for Q&A\"\n",
    "MODEL_NAME = \"BERT_QA\"\n",
    "\n",
    "# Set up the paths\n",
    "MODEL_PATH = \"models:/BERT_QA\"\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7cb782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging module with desired format and level\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "# Create a logger for this notebook\n",
    "logger = logging.getLogger('deployment-notebook')\n",
    "logger.info(\"Logging configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a2773-0c85-4753-af31-f78dbd4af094",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this part of the code, we load a Transformer model saved on Hugging Face to use it locally (in a pipeline object). This pipeline is then tested with a simple sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946866e-3fb0-4255-8d3f-1ecf7b52371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_PERSONAL_NAME\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=model_name,\n",
    "    device=0 # -1 means running on CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85e68f-7172-4eab-a7da-1dfae4d5a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline (context=\"Take me down to Paradise City where the grass is green and the girls are pretty\", question=\"What colour is the grass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73242074-aaab-4a8b-bd6e-75f2e54ce58d",
   "metadata": {},
   "source": [
    "This class below encapsulates the model in the format that will be logged/registered into MLFlow. It receives a pipeline (or a trainer) as input, saves the model into a temporary folder (called model_name), and log as an artifact into MLFlow. When MLFlow deploys the model, it loads these artifacts into a new pipeline, which can be used to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ffd49-bcfe-40e8-b871-b1bbd111d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTModel(mlflow.pyfunc.PythonModel):\n",
    "    def _preprocess(self, inputs):\n",
    "        \"\"\"\n",
    "        Preprocesses the input data.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict): A dictionary containing two keys:\n",
    "                - 'context' (list of str): A list with the context text.\n",
    "                - 'question' (list of str): A list with the question to be answered.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the context (str) and the question (str).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            context = inputs['context'][0]\n",
    "            question = inputs['question'][0]\n",
    "            logger.info(\"Preprocessing input:\", context, question)\n",
    "            return context, question\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing the input data: {str(e)}\")  \n",
    "\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the question-answering pipeline using the saved model artifact.\n",
    "\n",
    "        Args:\n",
    "            context (mlflow.pyfunc.PythonModelContext): The MLflow context object \n",
    "                containing the loaded artifacts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = pipeline(\n",
    "                'question-answering',\n",
    "                model=context.artifacts[\"model\"],\n",
    "                device=0\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading the question-answering pipeline: {str(e)}\")     \n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Runs inference using the loaded model and input data.\n",
    "\n",
    "        Args:\n",
    "            context (mlflow.pyfunc.PythonModelContext): The MLflow context object \n",
    "                with access to artifacts.\n",
    "            model_input (dict): A dictionary containing 'context' and 'question' keys.\n",
    "\n",
    "        Returns:\n",
    "            dict: The output from the model containing the predicted answer and optionally the score.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            in_ctx, question = self._preprocess(model_input)\n",
    "            output = self.model(context=in_ctx, question=question)\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error running inference: {str(e)}\")  \n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name, source_trainer=None, source_pipeline=None, demo_folder=\"demo\"):\n",
    "        \"\"\"\n",
    "        Logs the model to MLflow, including artifacts, dependencies, and input/output signatures.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Path where the model will be temporarily saved before logging.\n",
    "            source_trainer (optional): A trainer object with a `.save_model()` method. Defaults to None.\n",
    "            source_pipeline (optional): A pipeline object with a `.save_pretrained()` method. Defaults to None.\n",
    "            demo_folder (str): Path to the folder containing the compiled demo UI. Defaults to \"demo\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_schema = Schema([\n",
    "                ColSpec(\"string\", \"context\"),\n",
    "                ColSpec(\"string\", \"question\"),\n",
    "            ])\n",
    "\n",
    "            output_schema = Schema([\n",
    "                ColSpec(\"string\", \"answer\")\n",
    "            ])\n",
    "\n",
    "            params_schema = ParamSchema([\n",
    "                ParamSpec(\"show_score\", \"boolean\", False)\n",
    "            ])\n",
    "\n",
    "            signature = ModelSignature(\n",
    "                inputs=input_schema,\n",
    "                outputs=output_schema,\n",
    "                params=params_schema\n",
    "            )\n",
    "\n",
    "            # Save the model locally from the trainer or pipeline\n",
    "            if source_trainer is not None:\n",
    "                source_trainer.save_model(model_name)\n",
    "            elif source_pipeline is not None:\n",
    "                source_pipeline.save_pretrained(model_name)\n",
    "\n",
    "            # List of dependencies to include in the environment\n",
    "            requirements = [\n",
    "                \"transformers==4.47.0\",\n",
    "                \"tf_keras\"\n",
    "            ]\n",
    "\n",
    "            # Log the model to MLflow\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_name,\n",
    "                python_model=cls(),\n",
    "                artifacts={\n",
    "                    \"model\": model_name,\n",
    "                    \"demo\": demo_folder\n",
    "                },\n",
    "                signature=signature,\n",
    "                pip_requirements=requirements\n",
    "            )\n",
    "\n",
    "            # Remove the temporary model folder\n",
    "            shutil.rmtree(model_name)\n",
    "            logger.info(\"Logging model to MLflow done successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model to MLflow: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a56176-f1e7-4c64-b330-a36171fd79ad",
   "metadata": {},
   "source": [
    "# Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e7577-3629-45bd-8015-afd01713ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name = EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b60fd4-882f-4b9a-ad98-2bf722360af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name= MODEL_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    DistilBERTModel.log_model(model_name = MODEL_NAME, source_pipeline=qa_pipeline)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{MODEL_NAME}\", name = MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacc300-5e3d-4198-ac3d-f012c98f34cf",
   "metadata": {},
   "source": [
    "# Testing latest model registred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32c13e-2dad-484d-9990-bd8b3f38a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(MODEL_NAME, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "logger.info(latest_model_version, mlflow.models.get_model_info(f\"{MODEL_PATH}/{latest_model_version}\").signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476bd9b-15f0-4f6c-a789-11c24015ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"{MODEL_PATH}/{latest_model_version}\")\n",
    "context = \"Marta is mother of John and Amanda\"\n",
    "question = \"what is the name of Marta's daugther?\"\n",
    "model.predict({\"context\": [context], \"question\":[question]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f81b1ab",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
