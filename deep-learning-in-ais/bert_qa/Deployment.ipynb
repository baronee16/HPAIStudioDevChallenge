{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0c2be1-7e41-4318-b54b-74c0860ac597",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b853f387-e023-46f8-bb01-a7db1049e1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 19:45:20.246292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733859920.315975     314 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733859920.336587     314 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 19:45:20.536064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a2773-0c85-4753-af31-f78dbd4af094",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "In this part of the code, we load a Transformer model saved on Hugging Face to use it locally (in a pipeline object). This pipeline is then tested with a simple sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7946866e-3fb0-4255-8d3f-1ecf7b52371d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc0f58422de4e82bb11785f17cc7ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469e38e450fe47fdbc416c022bc51113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfa095912ca4813a50e195a923fe56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163659c56b164cb9975d35fbae12fe40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e680a6f5d4944d6993ac96c16d72d3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/669k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a8043c42ed4001a2b69fc211069e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"morgana-rodrigues/bert_qa\"\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    'question-answering',\n",
    "    model=model_name,\n",
    "    device=0 # -1 means running on CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e85e68f-7172-4eab-a7da-1dfae4d5a20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9356580972671509, 'start': 49, 'end': 54, 'answer': 'green'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pipeline (context=\"Take me down to Paradise City where the grass is green and the girls are pretty\", question=\"What colour is the grass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73242074-aaab-4a8b-bd6e-75f2e54ce58d",
   "metadata": {},
   "source": [
    "The class below encapsulates the model in the format that will be logged/registered into MLFlow. It receives a pipeline (or a trainer) as input, saves the model into a temporary folder (called model_name), and log as an artifact into MLFlow. When MLFlow deploys the model, it loads these artifacts into a new pipeline, which can be used to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764ffd49-bcfe-40e8-b871-b1bbd111d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTModel(mlflow.pyfunc.PythonModel):\n",
    "    def _preprocess(self, inputs):\n",
    "        context = inputs['context'][0]\n",
    "        question = inputs['question'][0]\n",
    "        print(\"pre processing\", context,question)\n",
    "        return context, question\n",
    "        \n",
    "    def load_context(self, context):\n",
    "        self.model = pipeline(\n",
    "            'question-answering',\n",
    "             model=context.artifacts[\"model\"],\n",
    "             device=0\n",
    "        )\n",
    "        \n",
    "    def predict(self, context, model_input, params):\n",
    "        in_ctx, question = self._preprocess(model_input)\n",
    "        output = self.model(context=in_ctx, question=question)\n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_name, source_trainer = None, source_pipeline = None, demo_folder=\"demo\"): \n",
    "        import shutil\n",
    "        input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"context\"),\n",
    "                ColSpec(\"string\", \"question\"),\n",
    "            ]\n",
    "        )\n",
    "        output_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"answer\")\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        params_schema = ParamSchema(\n",
    "            [\n",
    "                ParamSpec(\"show_score\", \"boolean\", False)\n",
    "            ]\n",
    "        )\n",
    "      \n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=params_schema)\n",
    "        if source_trainer is not None:\n",
    "            source_trainer.save_model(model_name)\n",
    "        elif source_pipeline is not None:\n",
    "            source_pipeline.save_pretrained(model_name)\n",
    "             \n",
    "        requirements = [\n",
    "            \"transformers==4.47.0\",\n",
    "            \"tf_keras\"\n",
    "        ]\n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_name,\n",
    "            python_model=cls(),\n",
    "            artifacts={\"model\": model_name, \"demo\": demo_folder},\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements\n",
    "        )\n",
    "        shutil.rmtree(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a56176-f1e7-4c64-b330-a36171fd79ad",
   "metadata": {},
   "source": [
    "## Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068e7577-3629-45bd-8015-afd01713ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/10 19:48:50 INFO mlflow.tracking.fluent: Experiment with name 'BERT model for Q&A' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/688720073654676786', creation_time=1733860130877, experiment_id='688720073654676786', last_update_time=1733860130877, lifecycle_stage='active', name='BERT model for Q&A', tags={}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name='BERT model for Q&A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b60fd4-882f-4b9a-ad98-2bf722360af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run's Artifact URI: /phoenix/mlflow/688720073654676786/b71faf115e81473faa9db442c0523df7/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e64a8d1c5b4a4c88921a8659dabbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0de11907ff49b88e3da63ea9b392c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'BERT_QA'.\n",
      "Created version '1' of model 'BERT_QA'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='BERT_QA') as run:\n",
    "    print(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    DistilBERTModel.log_model(model_name='BERT_QA', source_pipeline=qa_pipeline)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/BERT_QA\", name='BERT_QA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacc300-5e3d-4198-ac3d-f012c98f34cf",
   "metadata": {},
   "source": [
    "## Testing latest model registred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a32c13e-2dad-484d-9990-bd8b3f38a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 inputs: \n",
      "  ['context': string, 'question': string]\n",
      "outputs: \n",
      "  ['answer': string]\n",
      "params: \n",
      "  ['show_score': boolean (default: False)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(\"BERT_QA\", stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "print(latest_model_version, mlflow.models.get_model_info(f\"models:/BERT_QA/{latest_model_version}\").signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b476bd9b-15f0-4f6c-a789-11c24015ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/22 19:43:23 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - transformers (current: 4.39.1, required: transformers==4.37.0)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre processing ['Marta is mother of John and Amanda'] [\"what is the name of Marta's daugther?\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6202739477157593, 'start': 28, 'end': 34, 'answer': 'Amanda'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/BERT_QA/{latest_model_version}\")\n",
    "context = \"Marta is mother of John and Amanda\"\n",
    "question = \"what is the name of Marta's daugther?\"\n",
    "model.predict({\"context\": [context], \"question\":[question]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017bb69-3ae0-43c3-b411-3842d90c42eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aistudio]",
   "language": "python",
   "name": "conda-env-aistudio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
