{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcF9ZWvjSybR"
   },
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\">Data Analysis with Pandas and cuDF</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv0guwQAcpgX"
   },
   "source": [
    "In this notebook, we run a series of database operations using standard Pandas and cuDF, running on GPU. These values will be logged into MLFlow and used as refecence to compare with the CPU version. \n",
    "\n",
    "The data we'll be working with is a subset of the [USA 514 Stocks Prices NASDAQ NYSE dataset](https://www.kaggle.com/datasets/olegshpagin/usa-stocks-prices-ohlcv) from Kaggle. This was segmented in differently sized samples, with 5M, 10M, 15M and 20M data entries, and should be set up as an asset (Dataset) called USA_Stocks on the AI Studio project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Perform Analysis with Standard Pandas\n",
    "- Log Results to MLflow Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Load cuDF Pandas extension (required for GPU acceleration)\n",
    "# ========================================================\n",
    "%load_ext cudf.pandas\n",
    "\n",
    "# =============================\n",
    "# Standard Library Imports\n",
    "# =============================\n",
    "import os\n",
    "import time               # For runtime measurement (wall clock)\n",
    "import logging            # For application-level logging\n",
    "import warnings           # To manage and filter Python warnings\n",
    "from pathlib import Path  # For object-oriented filesystem paths\n",
    "\n",
    "# =============================\n",
    "# Third-Party Library Imports\n",
    "# =============================\n",
    "import pandas as pd       # Data manipulation and analysis\n",
    "import mlflow             # Experiment tracking and model logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Suppress Verbose Logs ------------------------\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"data_analysis_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\")  \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the USA stock parquet datasets\n",
    "DATASET_DIR = Path(\"/home/jovyan/datafabric/USA_Stocks/\")\n",
    "\n",
    "# Sample sizes (in millions of rows) to evaluate during the analysis\n",
    "SAMPLE_SIZES_TO_TEST = [5, 10]\n",
    "\n",
    "# Rolling window size (in days) used for time-series statistical operations\n",
    "ROLLING_WINDOW_SIZE = 7\n",
    "\n",
    "# Name of the MLflow experiment for tracking performance and metrics\n",
    "MLFLOW_EXPERIMENT_NAME = \"USA Stock Analysis with cuDF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 20:03:29 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmOguzNUcw4F",
    "outputId": "dbe56095-403b-4527-8809-472c0561d403"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 20:03:29 - INFO - Dataset is properly configured\n"
     ]
    }
   ],
   "source": [
    "# Define required dataset filenames\n",
    "dataset_filenames = [\n",
    "    \"usa_stocks_5m.parquet\",\n",
    "    \"usa_stocks_10m.parquet\",\n",
    "    \"usa_stocks_15m.parquet\",\n",
    "    \"usa_stocks_20m.parquet\",\n",
    "]\n",
    "\n",
    "# Construct full dataset file paths using pathlib\n",
    "dataset_paths = [DATASET_DIR / filename for filename in dataset_filenames]\n",
    "\n",
    "# Check if all dataset files exist\n",
    "all_files_exist = all(path.exists() for path in dataset_paths)\n",
    "\n",
    "# Output the dataset configuration status\n",
    "if all_files_exist:\n",
    "    logger.info(\"Dataset is properly configured\")\n",
    "else:\n",
    "    logger.info(\"Dataset is not properly configured. Please, create and download the assets on your project on AI Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pq01z9FvJjxR"
   },
   "source": [
    "# Perform Analysis with cuDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cells, we will define functions to run different operations in datasets:\n",
    "  * A function to describe the dataset\n",
    "  * A function to aggregate results grouped by \"ticker\" (the identifier of each stock)\n",
    "  * A function to aggregate by ticker, year and week\n",
    "  * A function to retrieve a rolling window with a given number of days for each ticker\n",
    "\n",
    "For each of these functions, the result will be displayed, and the necessary time to run will be logged into MLFlow. These functions will then be applied to the given set of samples in sample_sizes (e.g. [5, 10]). Bigger samples (15M and 20M) might be too heavy depending on the setup of your computer, so we recommend to configure the desired sample sizes according to the available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "DqqjcfcfJnvy",
    "outputId": "2592a7f6-8777-4515-9737-4e0c48228dc8"
   },
   "outputs": [],
   "source": [
    "def describe_dataframe(df):\n",
    "    \"\"\"\n",
    "    Compute basic descriptive statistics for the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, descriptive_statistics)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    descriptive_stats = df.describe()\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, descriptive_stats\n",
    "\n",
    "\n",
    "def aggregate_by_ticker(df):\n",
    "    \"\"\"\n",
    "    Perform simple aggregation grouped by ticker.\n",
    "\n",
    "    Aggregates:\n",
    "        - Minimum datetime\n",
    "        - Maximum datetime\n",
    "        - Count of records\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, aggregated_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    aggregation_result = df.groupby(\"ticker\").agg({\n",
    "        \"datetime\": [\"min\", \"max\", \"count\"]\n",
    "    })\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, aggregation_result\n",
    "\n",
    "\n",
    "def aggregate_by_ticker_week(df):\n",
    "    \"\"\"\n",
    "    Perform composite aggregation grouped by ticker, year, and week.\n",
    "\n",
    "    Aggregates:\n",
    "        - Minimum closing price\n",
    "        - Maximum closing price\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, aggregated_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    df[[\"year\", \"week\", \"day\"]] = df[\"datetime\"].dt.isocalendar()\n",
    "    aggregation_result = df.groupby([\"ticker\", \"year\", \"week\"]).agg({\n",
    "        \"close\": [\"min\", \"max\"]\n",
    "    })\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, aggregation_result\n",
    "\n",
    "\n",
    "def compute_rolling_mean(df, window_days):\n",
    "    \"\"\"\n",
    "    Calculate rolling window mean for each ticker over a given number of days.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        window_days (int): Number of days for the rolling window.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (elapsed_time_in_seconds, result_dataframe)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = (\n",
    "        df.set_index(\"datetime\")\n",
    "          .sort_index()\n",
    "          .groupby(\"ticker\")\n",
    "          .rolling(f\"{window_days}D\")\n",
    "          .mean()\n",
    "          .reset_index()\n",
    "    )\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return elapsed_time, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Results to MLflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Analysis for 5M Rows ---\n",
      "Description Time      : 7.4010 seconds\n",
      "Simple Aggregation    : 5.0535 seconds\n",
      "Composite Aggregation : 5.5397 seconds\n",
      "Rolling Window (7D) : 23.3762 seconds\n",
      "\n",
      "--- Running Analysis for 10M Rows ---\n",
      "Description Time      : 7.5015 seconds\n",
      "Simple Aggregation    : 10.7850 seconds\n",
      "Composite Aggregation : 8.8761 seconds\n",
      "Rolling Window (7D) : 76.5552 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set the MLflow experiment to track runs\n",
    "mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "# Loop through each dataset sample size and run analysis\n",
    "for sample_size in SAMPLE_SIZES_TO_TEST:\n",
    "    run_name = f\"cuDF Analysis - {sample_size}M\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Log configuration parameters\n",
    "        mlflow.log_param(\"Computing\", \"gpu\")\n",
    "        mlflow.log_param(\"Dataset size in millions of rows\", sample_size)\n",
    "        \n",
    "        # Load dataset corresponding to the current sample size\n",
    "        dataset_path = f\"/home/jovyan/datafabric/USA_Stocks/usa_stocks_{sample_size}m.parquet\"\n",
    "        df = pd.read_parquet(dataset_path)\n",
    "\n",
    "        print(f\"\\n--- Running Analysis for {sample_size}M Rows ---\")\n",
    "        \n",
    "        # Description\n",
    "        description_time, _ = describe_dataframe(df)\n",
    "        mlflow.log_metric(\"Description_time_seconds\", description_time)\n",
    "        print(f\"Description Time      : {description_time:.4f} seconds\")\n",
    "        \n",
    "        # Simple Aggregation\n",
    "        simple_agg_time, _ = aggregate_by_ticker(df)\n",
    "        mlflow.log_metric(\"Simple_aggregation_time_seconds\", simple_agg_time)\n",
    "        print(f\"Simple Aggregation    : {simple_agg_time:.4f} seconds\")\n",
    "        \n",
    "        # Composite Aggregation\n",
    "        composite_agg_time, _ = aggregate_by_ticker_week(df)\n",
    "        mlflow.log_metric(\"Composite_aggregation_time_seconds\", composite_agg_time)\n",
    "        print(f\"Composite Aggregation : {composite_agg_time:.4f} seconds\")\n",
    "        \n",
    "        # Rolling Window\n",
    "        rolling_time, _ = compute_rolling_mean(df, ROLLING_WINDOW_SIZE)\n",
    "        mlflow.log_metric(f\"Rolling_window_{ROLLING_WINDOW_SIZE}D_time_seconds\", rolling_time)\n",
    "        print(f\"Rolling Window ({ROLLING_WINDOW_SIZE}D) : {rolling_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-08 20:06:15 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
